{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "###Default for jupyter display\n",
    "matplotlib.rcParams['figure.dpi']= 150\n",
    "\n",
    "matplotlib.rcParams['xtick.labelsize'] = 7\n",
    "matplotlib.rcParams['ytick.labelsize'] = 7\n",
    "matplotlib.rcParams['axes.labelsize'] = 7\n",
    "matplotlib.rcParams['axes.titlesize'] = 7\n",
    "\n",
    "matplotlib.rcParams['axes.grid'] = True\n",
    "matplotlib.rcParams['grid.color'] = '0.8'\n",
    "matplotlib.rcParams['grid.linewidth'] = '0.5'\n",
    "\n",
    "matplotlib.rcParams['axes.edgecolor'] = '0.25'\n",
    "matplotlib.rcParams['xtick.color'] = '0'\n",
    "matplotlib.rcParams['ytick.color'] = '0'\n",
    "\n",
    "matplotlib.rcParams['xtick.major.width'] = 1\n",
    "matplotlib.rcParams['ytick.major.width'] = 1\n",
    "matplotlib.rcParams['ytick.major.size'] = 5\n",
    "matplotlib.rcParams['xtick.major.size'] = 5\n",
    "matplotlib.rcParams['axes.spines.right'] = True\n",
    "matplotlib.rcParams['axes.spines.left'] = True\n",
    "matplotlib.rcParams['axes.spines.top'] = True\n",
    "matplotlib.rcParams['axes.spines.bottom'] = True\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "matplotlib.rcParams['font.weight']='normal'\n",
    "matplotlib.rcParams['axes.axisbelow'] = True\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "matplotlib.rcParams['legend.fontsize'] = 7\n",
    "matplotlib.rcParams['legend.handlelength'] = 1\n",
    "matplotlib.rcParams['legend.handleheight'] = 1\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.4\n",
    "\n",
    "\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in / process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(634, 229)\n",
      "(423, 229)\n"
     ]
    }
   ],
   "source": [
    "###Load classifier model\n",
    "clf = joblib.load('../Data/classifier_data/rf_version_updating.joblib')\n",
    "\n",
    "###Load datasets\n",
    "train_df = pd.read_csv('../Data/classifier_data/train_df.csv', index_col=0)\n",
    "print(train_df.shape)\n",
    "test_df = pd.read_csv('../Data/classifier_data/test_df.csv', index_col=0)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate my predictions into the training/testing dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['my_predictions'] = clf.predict(train_df[train_df.columns[23:]])\n",
    "test_df['my_predictions'] = clf.predict(test_df[test_df.columns[23:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['my_predictions'] = train_df['my_predictions'].replace(1, 'yes')\n",
    "train_df['my_predictions'] = train_df['my_predictions'].replace(0, 'no')\n",
    "\n",
    "test_df['my_predictions'] = test_df['my_predictions'].replace(1, 'yes')\n",
    "test_df['my_predictions'] = test_df['my_predictions'].replace(0, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate PHACTS predictions into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    listy = []\n",
    "    for index in df.index[:]:\n",
    "        name = df.loc[index]['Identifier_AJH']\n",
    "        phacts_out = '../Data/phage_data_nmicro2017/PHACTS_results/{}_prodigal.out'.format(name)\n",
    "        name2 = phacts_out.split('/')[-1].split('_prodigal.out')[0]\n",
    "        assert name == name2\n",
    "        phacts_df = pd.read_csv(phacts_out, sep='\\t', skiprows=3, header=None)\n",
    "        assert phacts_df.iloc[0][1] >= phacts_df.iloc[1][1]\n",
    "        if phacts_df.iloc[0][1] == phacts_df.iloc[1][1]:\n",
    "            listy.append('-')\n",
    "            continue\n",
    "        if phacts_df.iloc[0][0] == 'Temperate':\n",
    "            listy.append('yes')\n",
    "        else:\n",
    "            listy.append('no')\n",
    "    df['Temperate_PHACTS'] = listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of a total 634\n",
      "My method had 1\n",
      "Other method had 24\n",
      "PHACTS method had 120\n"
     ]
    }
   ],
   "source": [
    "###Training set\n",
    "my_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['my_predictions']].shape[0]\n",
    "\n",
    "mavrich_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['Temperate (bioinformatically predicted)']].shape[0]\n",
    "\n",
    "phacts_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['Temperate_PHACTS']].shape[0]\n",
    "\n",
    "print('Out of a total {}'.format(train_df.shape[0]))\n",
    "print('My method had {}'.format(my_train_errors))\n",
    "print('Other method had {}'.format(mavrich_train_errors))\n",
    "print('PHACTS method had {}'.format(phacts_train_errors))\n",
    "\n",
    "my_train_errors = my_train_errors / train_df.shape[0] * 100\n",
    "mavrich_train_errors = mavrich_train_errors / train_df.shape[0] * 100\n",
    "phacts_train_errors = phacts_train_errors / train_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of a total 423\n",
      "My method had 7\n",
      "Other method had 19\n",
      "PHACTS method had 89\n"
     ]
    }
   ],
   "source": [
    "###Test set\n",
    "my_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['my_predictions']].shape[0]\n",
    "\n",
    "mavrich_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['Temperate (bioinformatically predicted)']].shape[0]\n",
    "\n",
    "phacts_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['Temperate_PHACTS']].shape[0]\n",
    "\n",
    "print('Out of a total {}'.format(test_df.shape[0]))\n",
    "print('My method had {}'.format(my_test_errors))\n",
    "print('Other method had {}'.format(mavrich_test_errors))\n",
    "print('PHACTS method had {}'.format(phacts_test_errors))\n",
    "\n",
    "my_test_errors = my_test_errors / test_df.shape[0] * 100\n",
    "mavrich_test_errors = mavrich_test_errors / test_df.shape[0] * 100\n",
    "phacts_test_errors = phacts_test_errors / test_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550827423167849"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(19/423)\n",
    "# 1-(1/634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further split the test set into easy/difficult sets\n",
    "\n",
    "**According to some pre-calculated clusters of data, this splits the test set up specifically according to whether or not a related sequence (from the same cluster) was / was not included in the training set. The goal is to assess accuracy on an un-polluted (as much as possible) test set**\n",
    "\n",
    "See `cluster_seqs.ipynb` for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../Data/fastANI_output/clusters.json', 'r') as infile:\n",
    "    clusters = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_names = list(train_df['Identifier_AJH'])\n",
    "independent_set = []\n",
    "related_set = []\n",
    "for cluster in clusters:\n",
    "    hits = []\n",
    "    for member in cluster:\n",
    "        if member in temp_names:\n",
    "            hits.append(member)\n",
    "    if len(hits) == 0:\n",
    "        independent_set.extend(cluster)\n",
    "    else:\n",
    "        related_set.extend(cluster)\n",
    "###Get the challenging/easy sets        \n",
    "independent_df = test_df[test_df['Identifier_AJH'].isin(independent_set)]\n",
    "print(independent_df.shape)\n",
    "related_df = test_df[test_df['Identifier_AJH'].isin(related_set)]\n",
    "print(related_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['my_predictions']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "mavrich_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['Temperate (bioinformatically predicted)']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "phacts_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['Temperate_PHACTS']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "\n",
    "\n",
    "my_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['my_predictions']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100.\n",
    "\n",
    "mavrich_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['Temperate (bioinformatically predicted)']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100.\n",
    "\n",
    "phacts_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['Temperate_PHACTS']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100-phacts_independent_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a final plot of the error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "my_errs = [my_train_errors, my_test_errors, my_related_errors, my_independent_errors][::-1]\n",
    "mavrich_errs = [mavrich_train_errors, mavrich_test_errors, mavrich_related_errors, mavrich_independent_errors][::-1]\n",
    "phacts_errs = [phacts_train_errors, phacts_test_errors, phacts_related_errors, phacts_independent_errors][::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.7,1.7))\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.25         # the width of the bars\n",
    "p1 = ax.barh(ind, phacts_errs, width)\n",
    "p2 = ax.barh(ind+width, mavrich_errs, width)\n",
    "p3 = ax.barh(ind+width+width, my_errs, width)\n",
    "\n",
    "\n",
    "ax.set_yticks(0.25 + np.arange(N))\n",
    "ax.set_yticklabels(['Training set\\n(n={})'.format(train_df.shape[0]),\\\n",
    "                    'Testing set\\n(n={})'.format(test_df.shape[0]),\\\n",
    "                    'Testing set, rel.\\n(n={})'.format(related_df.shape[0]),\\\n",
    "                    'Testing set, ind.\\n(n={})'.format(independent_df.shape[0])][::-1],\\\n",
    "                   ha='center', )\n",
    "ax.yaxis.get_majorticklabels()[0].set_x(-0.17)\n",
    "ax.yaxis.get_majorticklabels()[1].set_x(-0.17)\n",
    "ax.yaxis.get_majorticklabels()[2].set_x(-0.17)\n",
    "ax.yaxis.get_majorticklabels()[3].set_x(-0.17)\n",
    "\n",
    "\n",
    "\n",
    "ax.legend((p3[0], p2[0], p1[0]), ('BACPHLIP', 'Mavrich', 'PHACTS'),\\\n",
    "          bbox_to_anchor=(0., 1.02, 1., .102), loc=3, mode='expand', ncol=3, borderaxespad=0.)\n",
    "\n",
    "ax.set_xlabel('% Incorrect predictions');\n",
    "plt.savefig('../Manuscript/figure.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices and other assorted metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score, accuracy_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns[:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400/423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df\n",
    "print(temp_df['Temperate (empirical)'].value_counts())\n",
    "print()\n",
    "print('BACPHLIP')\n",
    "print(confusion_matrix(temp_df['Temperate (empirical)'], temp_df['my_predictions']))\n",
    "print()\n",
    "print('Mavrich')\n",
    "print(confusion_matrix(temp_df['Temperate (empirical)'], temp_df['Temperate (bioinformatically predicted)']))\n",
    "print()\n",
    "print('PHACTS')\n",
    "print(confusion_matrix(temp_df['Temperate (empirical)'], temp_df['Temperate_PHACTS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df\n",
    "print(temp_df['Temperate (empirical)'].value_counts())\n",
    "print()\n",
    "print('BACPHLIP')\n",
    "print(accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['my_predictions'].map(dict(yes=1, no=0))))\n",
    "print()\n",
    "print('Mavrich')\n",
    "print(accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate (bioinformatically predicted)'].map(dict(yes=1, no=0))))\n",
    "print()\n",
    "print('PHACTS')\n",
    "print(accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate_PHACTS'].map(dict(yes=1, no=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df\n",
    "print(temp_df['Temperate (empirical)'].value_counts())\n",
    "print()\n",
    "print('BACPHLIP')\n",
    "print(balanced_accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['my_predictions'].map(dict(yes=1, no=0)), adjusted=True))\n",
    "print()\n",
    "print('Mavrich')\n",
    "print(balanced_accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate (bioinformatically predicted)'].map(dict(yes=1, no=0)), adjusted=True))\n",
    "print()\n",
    "print('PHACTS')\n",
    "print(balanced_accuracy_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate_PHACTS'].map(dict(yes=1, no=0)), adjusted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df\n",
    "print(temp_df['Temperate (empirical)'].value_counts())\n",
    "print()\n",
    "print('BACPHLIP')\n",
    "print(f1_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['my_predictions'].map(dict(yes=1, no=0))))\n",
    "\n",
    "print()\n",
    "print('Mavrich')\n",
    "print(f1_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate (bioinformatically predicted)'].map(dict(yes=1, no=0))))\n",
    "\n",
    "print()\n",
    "print('PHACTS')\n",
    "print(f1_score(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate_PHACTS'].map(dict(yes=1, no=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df\n",
    "print(temp_df['Temperate (empirical)'].value_counts())\n",
    "print()\n",
    "print('BACPHLIP')\n",
    "print(matthews_corrcoef(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['my_predictions'].map(dict(yes=1, no=0))))\n",
    "\n",
    "print()\n",
    "print('Mavrich')\n",
    "print(matthews_corrcoef(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate (bioinformatically predicted)'].map(dict(yes=1, no=0))))\n",
    "\n",
    "print()\n",
    "print('PHACTS')\n",
    "print(matthews_corrcoef(temp_df['Temperate (empirical)'].map(dict(yes=1, no=0)),\\\n",
    "               temp_df['Temperate_PHACTS'].map(dict(yes=1, no=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for differences related to genome size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df[test_df['my_predictions'] != test_df['Temperate (empirical)']]\n",
    "print(temp_df.shape)\n",
    "incorrect_sizes = temp_df['Genome size']\n",
    "\n",
    "temp_df = test_df[test_df['my_predictions'] == test_df['Temperate (empirical)']]\n",
    "print(temp_df.shape)\n",
    "correct_sizes = temp_df['Genome size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(incorrect_sizes), np.mean(correct_sizes))\n",
    "print(np.median(incorrect_sizes), np.median(correct_sizes))\n",
    "print(stats.ranksums(incorrect_sizes, correct_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High confidence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =  clf.predict_proba(test_df[test_df.columns[23:-1]])\n",
    "preds = np.array(list(zip(*preds))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors on high confidence predictions:\n",
      "1\n",
      "Total high confidence predictions:\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "###Test set\n",
    "print('Errors on high confidence predictions:')\n",
    "print(test_df[(test_df['Temperate (empirical)'] != \n",
    "                           test_df['my_predictions']) &\\\n",
    "                         ((preds>=0.95) | (preds<=0.05))].shape[0])\n",
    "print('Total high confidence predictions:')\n",
    "print(test_df[((preds>=0.95) | (preds<=0.05))].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    201\n",
       "no     132\n",
       "Name: my_predictions, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[((preds>=0.95) | (preds<=0.05))]['my_predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
