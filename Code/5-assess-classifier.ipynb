{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "###Default for jupyter display\n",
    "matplotlib.rcParams['figure.dpi']= 150\n",
    "\n",
    "matplotlib.rcParams['xtick.labelsize'] = 7\n",
    "matplotlib.rcParams['ytick.labelsize'] = 7\n",
    "matplotlib.rcParams['axes.labelsize'] = 7\n",
    "matplotlib.rcParams['axes.titlesize'] = 7\n",
    "\n",
    "matplotlib.rcParams['axes.grid'] = True\n",
    "matplotlib.rcParams['grid.color'] = '0.8'\n",
    "matplotlib.rcParams['grid.linewidth'] = '0.5'\n",
    "\n",
    "matplotlib.rcParams['axes.edgecolor'] = '0.25'\n",
    "matplotlib.rcParams['xtick.color'] = '0'\n",
    "matplotlib.rcParams['ytick.color'] = '0'\n",
    "\n",
    "matplotlib.rcParams['xtick.major.width'] = 1\n",
    "matplotlib.rcParams['ytick.major.width'] = 1\n",
    "matplotlib.rcParams['ytick.major.size'] = 5\n",
    "matplotlib.rcParams['xtick.major.size'] = 5\n",
    "matplotlib.rcParams['axes.spines.right'] = True\n",
    "matplotlib.rcParams['axes.spines.left'] = True\n",
    "matplotlib.rcParams['axes.spines.top'] = True\n",
    "matplotlib.rcParams['axes.spines.bottom'] = True\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "matplotlib.rcParams['font.weight']='normal'\n",
    "matplotlib.rcParams['axes.axisbelow'] = True\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "matplotlib.rcParams['legend.fontsize'] = 7\n",
    "matplotlib.rcParams['legend.handlelength'] = 1\n",
    "matplotlib.rcParams['legend.handleheight'] = 1\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.4\n",
    "\n",
    "\n",
    "\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in / process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load classifier model\n",
    "# clf = joblib.load('../Data/classifier_data/rf_best.joblib')\n",
    "clf = joblib.load('../Data/classifier_data/rf_highMinAJH.joblib')\n",
    "\n",
    "###Load datasets\n",
    "train_df = pd.read_csv('../Data/classifier_data/train_df.csv', index_col=0)\n",
    "print(train_df.shape)\n",
    "test_df = pd.read_csv('../Data/classifier_data/test_df.csv', index_col=0)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate my predictions into the training/testing dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['my_predictions'] = clf.predict(train_df[train_df.columns[23:]])\n",
    "test_df['my_predictions'] = clf.predict(test_df[test_df.columns[23:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['my_predictions'] = train_df['my_predictions'].replace(1, 'yes')\n",
    "train_df['my_predictions'] = train_df['my_predictions'].replace(0, 'no')\n",
    "\n",
    "test_df['my_predictions'] = test_df['my_predictions'].replace(1, 'yes')\n",
    "test_df['my_predictions'] = test_df['my_predictions'].replace(0, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrate PHACTS predictions into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    listy = []\n",
    "    for index in df.index[:]:\n",
    "        name = df.loc[index]['Identifier_AJH']\n",
    "        phacts_out = '../Data/phage_data_nmicro2017/PHACTS_results/{}_prodigal.out'.format(name)\n",
    "        name2 = phacts_out.split('/')[-1].split('_prodigal.out')[0]\n",
    "        assert name == name2\n",
    "        phacts_df = pd.read_csv(phacts_out, sep='\\t', skiprows=3, header=None)\n",
    "        assert phacts_df.iloc[0][1] >= phacts_df.iloc[1][1]\n",
    "        if phacts_df.iloc[0][1] == phacts_df.iloc[1][1]:\n",
    "            listy.append('-')\n",
    "            continue\n",
    "        if phacts_df.iloc[0][0] == 'Temperate':\n",
    "            listy.append('yes')\n",
    "        else:\n",
    "            listy.append('no')\n",
    "    df['Temperate_PHACTS'] = listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Training set\n",
    "my_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['my_predictions']].shape[0]\n",
    "\n",
    "other_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['Temperate (bioinformatically predicted)']].shape[0]\n",
    "\n",
    "phacts_train_errors = train_df[train_df['Temperate (empirical)'] != \n",
    "                           train_df['Temperate_PHACTS']].shape[0]\n",
    "\n",
    "print('Out of a total {}'.format(train_df.shape[0]))\n",
    "print('My method had {}'.format(my_train_errors))\n",
    "print('Other method had {}'.format(other_train_errors))\n",
    "print('PHACTS method had {}'.format(phacts_train_errors))\n",
    "\n",
    "my_train_errors = my_train_errors / train_df.shape[0] * 100\n",
    "other_train_errors = other_train_errors / train_df.shape[0] * 100\n",
    "phacts_train_errors = phacts_train_errors / train_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test set\n",
    "my_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['my_predictions']].shape[0]\n",
    "\n",
    "other_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['Temperate (bioinformatically predicted)']].shape[0]\n",
    "\n",
    "phacts_test_errors = test_df[test_df['Temperate (empirical)'] != \n",
    "                           test_df['Temperate_PHACTS']].shape[0]\n",
    "\n",
    "print('Out of a total {}'.format(test_df.shape[0]))\n",
    "print('My method had {}'.format(my_test_errors))\n",
    "print('Other method had {}'.format(other_test_errors))\n",
    "print('PHACTS method had {}'.format(phacts_test_errors))\n",
    "\n",
    "my_test_errors = my_test_errors / test_df.shape[0] * 100\n",
    "other_test_errors = other_test_errors / test_df.shape[0] * 100\n",
    "phacts_test_errors = phacts_test_errors / test_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(19/423)\n",
    "# 1-(1/634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further split the test set into easy/difficult sets\n",
    "\n",
    "**According to some pre-calculated clusters of data, this splits the test set up specifically according to whether or not a related sequence (from the same cluster) was / was not included in the training set. The goal is to assess accuracy on an un-polluted (as much as possible) test set**\n",
    "\n",
    "See `cluster_seqs.ipynb` for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../Data/fastANI_output/clusters.json', 'r') as infile:\n",
    "    clusters = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_names = list(train_df['Identifier_AJH'])\n",
    "independent_set = []\n",
    "related_set = []\n",
    "for cluster in clusters:\n",
    "    hits = []\n",
    "    for member in cluster:\n",
    "        if member in temp_names:\n",
    "            hits.append(member)\n",
    "    if len(hits) == 0:\n",
    "        independent_set.extend(cluster)\n",
    "    else:\n",
    "        related_set.extend(cluster)\n",
    "###Get the challenging/easy sets        \n",
    "independent_df = test_df[test_df['Identifier_AJH'].isin(independent_set)]\n",
    "print(independent_df.shape)\n",
    "related_df = test_df[test_df['Identifier_AJH'].isin(related_set)]\n",
    "print(related_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['my_predictions']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "other_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['Temperate (bioinformatically predicted)']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "phacts_independent_errors = independent_df[independent_df['Temperate (empirical)'] != \n",
    "                           independent_df['Temperate_PHACTS']].shape[0] /\\\n",
    "                            independent_df.shape[0] * 100.\n",
    "\n",
    "\n",
    "\n",
    "my_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['my_predictions']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100.\n",
    "\n",
    "other_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['Temperate (bioinformatically predicted)']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100.\n",
    "\n",
    "phacts_related_errors = related_df[related_df['Temperate (empirical)'] != \n",
    "                           related_df['Temperate_PHACTS']].shape[0] /\\\n",
    "                            related_df.shape[0] * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100-phacts_independent_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a final plot of the error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "my_errs = [my_train_errors, my_test_errors, my_related_errors, my_independent_errors][::-1]\n",
    "other_errs = [other_train_errors, other_test_errors, other_related_errors, other_independent_errors][::-1]\n",
    "phacts_errs = [phacts_train_errors, phacts_test_errors, phacts_related_errors, phacts_independent_errors][::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2.6,2.4))\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.25         # the width of the bars\n",
    "p1 = ax.barh(ind, phacts_errs, width)\n",
    "p2 = ax.barh(ind+width, other_errs, width)\n",
    "p3 = ax.barh(ind+width+width, my_errs, width)\n",
    "\n",
    "\n",
    "ax.set_yticks(0.25 + np.arange(N))\n",
    "ax.set_yticklabels(['Training set\\n(n={})'.format(train_df.shape[0]),\\\n",
    "                    'Testing set\\n(n={})'.format(test_df.shape[0]),\\\n",
    "                    'Testing set\\n(related, n={})'.format(related_df.shape[0]),\\\n",
    "                    'Testing set\\n(independent, n={})'.format(independent_df.shape[0])][::-1],\\\n",
    "                   ha='center', )\n",
    "ax.yaxis.get_majorticklabels()[0].set_x(-0.2)\n",
    "ax.yaxis.get_majorticklabels()[1].set_x(-0.2)\n",
    "ax.yaxis.get_majorticklabels()[2].set_x(-0.2)\n",
    "ax.yaxis.get_majorticklabels()[3].set_x(-0.2)\n",
    "\n",
    "\n",
    "\n",
    "ax.legend((p3[0], p2[0], p1[0]), ('BACPHLIP', 'Mavrich', 'PHACTS'),\\\n",
    "          bbox_to_anchor=(0., 1.02, 1., .102), loc=3, mode='expand', ncol=3, borderaxespad=0.)\n",
    "\n",
    "ax.set_xlabel('% Incorrect predictions');\n",
    "plt.savefig('../Manuscript/figure.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "ax.hist(clf.feature_importances_, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This should be 1:', sum(clf.feature_importances_))\n",
    "print('How many features were useless:', len([i for i in clf.feature_importances_ if i == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df = pd.read_csv('../Data/protein_domain_data/cddid_selected_2020_4_27.tsv', sep='\\t', index_col=0)\n",
    "print(domain_df.shape)\n",
    "domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippy = list(zip(clf.feature_importances_, train_df.columns[23:-2]))\n",
    "print(zippy[:5])\n",
    "non_zero_cols = [i[1] for i in zippy if i[0] != 0.0]\n",
    "print(len(non_zero_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_domain_df = domain_df[domain_df['1'].isin(non_zero_cols)==False]\n",
    "good_domain_df = domain_df[domain_df['1'].isin(non_zero_cols)==True]\n",
    "print(bad_domain_df.shape, good_domain_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_col in domain_df.columns[4:]:\n",
    "    print(test_col, bad_domain_df[test_col].sum(), good_domain_df[test_col].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at most important features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippy = list(zip(clf.feature_importances_, train_df.columns[23:-2]))\n",
    "zippy = sorted(zippy, key=lambda x: x[0], reverse=True)\n",
    "best_fams = [i[1] for i in zippy[:20]]\n",
    "best_domain_df = domain_df[domain_df['1'].isin(best_fams)==True]\n",
    "print(best_domain_df.shape)\n",
    "best_domain_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Those best 20 domains account for {} '\n",
    "      'of the overall feature importance'.format(np.sum([i[0] for i in zippy[:20]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(test_df[test_df.columns[23:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
