{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SearchIO\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_file = '../Data/phage_data_nmicro2017/processed_benchmark_set.csv'\n",
    "\n",
    "hmm_results_dir = '../Data/phage_data_nmicro2017/hmmsearch_out/'\n",
    "\n",
    "base_dir = '../Data/classifier_data/'\n",
    "\n",
    "hmm_data_dir = '../Data/protein_domain_data/domain_alignments_and_hmms/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting shape: (1057, 22)\n",
      "New shape (should be identical): (1057, 22)\n",
      "New shape (+1 new column): (1057, 23)\n"
     ]
    }
   ],
   "source": [
    "###Read in the dataset and double check that analysis is limited to empirically defined data\n",
    "df = pd.read_csv(benchmark_file, index_col=0)\n",
    "print('Starting shape:', df.shape)\n",
    "df = df[df['Temperate (empirical)'] != 'Unspecified']\n",
    "print('New shape (should be identical):', df.shape)\n",
    "\n",
    "###Add in my identifier\n",
    "df['Identifier_AJH'] = ''  \n",
    "df.at[df[df['Database source'] == 'NCBI RefSeq'].index, 'Identifier_AJH'] =\\\n",
    "                    df[df['Database source'] == 'NCBI RefSeq']['RefSeq accession number']\n",
    "df.at[df[df['Database source'] == 'Actinobacteriophage_785'].index, 'Identifier_AJH'] =\\\n",
    "                    df[df['Database source'] == 'Actinobacteriophage_785']['Virus identifier used for the analysis'].str.split('_').str[0]\n",
    "print('New shape (+1 new column):', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1057, 371)\n"
     ]
    }
   ],
   "source": [
    "###Read through all of the hmmsearch files to accumulate a growing presence/absence dataframe\n",
    "file_ending = '.fasta.hmmsearch'\n",
    "growing_df = pd.DataFrame()\n",
    "for index in df.index:\n",
    "    if df.loc[index]['Database source'] == 'NCBI RefSeq':\n",
    "        file_name = df.loc[index]['RefSeq accession number'] + file_ending\n",
    "    elif df.loc[index]['Database source'] == 'Actinobacteriophage_785':\n",
    "        file_name = df.loc[index]['Virus identifier used for the analysis'].split('_')[0].lower() +\\\n",
    "                    file_ending\n",
    "    try:\n",
    "        with open(hmm_results_dir + file_name, 'r') as infile:\n",
    "            results = list(SearchIO.parse(infile, 'hmmer3-text'))\n",
    "            simple_res = []\n",
    "            for i in results:\n",
    "                if len(i.hits) > 0:\n",
    "                    simple_res.append((i.id, 1))\n",
    "                else:\n",
    "                    simple_res.append((i.id, 0))\n",
    "        simple_res = sorted(simple_res, key=lambda x: x[0])\n",
    "        single_df = pd.DataFrame(OrderedDict(simple_res), index=[index])\n",
    "        growing_df = pd.concat([growing_df, single_df])\n",
    "    except FileNotFoundError:\n",
    "        print('Failed to find this file: {}!'.format(file_name))\n",
    "        pass\n",
    "print(growing_df.shape)\n",
    "\n",
    "###Add that to the main dataframe\n",
    "full_df = df.join(growing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training and testing dataframes: (634, 394) (423, 394)\n",
      "Shape of training and testing labels: (634,) (423,)\n"
     ]
    }
   ],
   "source": [
    "###Split into training and testing sets\n",
    "train_df, test_df = train_test_split(full_df, train_size=0.6,\\\n",
    "                                     random_state=42)\n",
    "print('Shape of training and testing dataframes:', train_df.shape, test_df.shape)\n",
    "\n",
    "###Set up the machine-learning training and test sets\n",
    "ml_df_train = train_df[train_df.columns[23:]]\n",
    "ml_df_test = test_df[test_df.columns[23:]]\n",
    "\n",
    "###And labels\n",
    "training_labels = pd.DataFrame(index=train_df.index)\n",
    "training_labels['binary'] = 0\n",
    "training_labels.at[train_df[train_df['Temperate (empirical)']=='yes'].index, 'binary'] = 1\n",
    "training_labels = training_labels['binary']\n",
    "\n",
    "testing_labels = pd.DataFrame(index=test_df.index)\n",
    "testing_labels['binary'] = 0\n",
    "testing_labels.at[test_df[test_df['Temperate (empirical)']=='yes'].index, 'binary'] = 1\n",
    "testing_labels = testing_labels['binary']\n",
    "\n",
    "print('Shape of training and testing labels:', training_labels.shape, testing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features that are likely to be noise\n",
    "\n",
    "My goal in semi-rationally selecting these protein domains was to *predict temperate phages*, ergo I don't even want a predictor of lytic phages in my dataset and given the choice of protein domains that I am including believe that these would likely be noise.\n",
    "\n",
    "**Note that this step is reasonable if I *only* consider the training set in making this decision, as I am doing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running count of uninformative columns: 55\n",
      "Running count of uninformative columns: 165\n"
     ]
    }
   ],
   "source": [
    "uninformative_cols = []\n",
    "###Use training set to remove certain cases with too few hits (likely unreliable/not useful)\n",
    "too_few_count = 2\n",
    "transpose_df = ml_df_train.transpose()\n",
    "uninformative_cols.extend(list(transpose_df[transpose_df.sum(axis=1)<=too_few_count].index))\n",
    "print('Running count of uninformative columns:', len(uninformative_cols))\n",
    "###Cases where lytic features are higher than or equal to temperate\n",
    "lysog_df = ml_df_train[train_df['Temperate (empirical)']=='yes'].transpose()\n",
    "lytic_df = ml_df_train[train_df['Temperate (empirical)']=='no'].transpose()\n",
    "uninformative_cols.extend(list(transpose_df[lysog_df.sum(axis=1) <= (lytic_df.sum(axis=1))].index))\n",
    "uninformative_cols = list(set(uninformative_cols))\n",
    "print('Running count of uninformative columns:', len(uninformative_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop these columns from train, test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (634, 206)\n",
      "Testing set shape (423, 206)\n"
     ]
    }
   ],
   "source": [
    "ml_df_train = ml_df_train.drop(columns=uninformative_cols)\n",
    "print('Training set shape', ml_df_train.shape)\n",
    "\n",
    "ml_df_test = ml_df_test.drop(columns=uninformative_cols)\n",
    "print('Testing set shape', ml_df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write training and testing dataframes to a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=uninformative_cols).to_csv(base_dir + 'train_df.csv')\n",
    "test_df.drop(columns=uninformative_cols).to_csv(base_dir + 'test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And write a new `.hmm` file containing only these *putatively* useful domains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = list(ml_df_train.columns)\n",
    "with open(base_dir+'best_models.hmm', 'w') as outfile:\n",
    "    for i in identifiers:\n",
    "        fname = hmm_data_dir+'{}.hmm'.format(i)\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "print(len(identifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning of a model\n",
    "\n",
    "This is using a very basic Random Forest classifier with grid search to optimize hyper parameters via cross-validation. I'm using/testing 2 different cross-validation strategies:\n",
    "\n",
    "    1. normal k-fold cross-validation\n",
    "        - where best is the highest mean amongst cross-val scores (standard approach)\n",
    "    2. more intense bootstrap sampling version\n",
    "        - where best is the highest minimum amongst cross-val scores (my interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of best model: {'bootstrap': False, 'class_weight': 'balanced', 'max_depth': 36, 'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "Cross validation scores using that model: [0.95714286 0.99310345 0.97931034 0.99310345 0.96503497]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../Data/classifier_data/rf_best.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Really simple random forest hyper-parameter sweep\n",
    "scoring_fxn = 'f1'\n",
    "n_fold_cv = 5\n",
    "#\n",
    "rf = RandomForestClassifier()\n",
    "params_rf = {'bootstrap': [True, False],\\\n",
    "             'class_weight':['balanced', 'balanced_subsample'],\\\n",
    "             'min_samples_leaf': [1, 2],\\\n",
    "             'n_estimators': list(range(10, 105, 5)),\\\n",
    "             'max_depth': list(range(10, 42, 2))}\n",
    "\n",
    "rf_gs = GridSearchCV(rf, params_rf, scoring=scoring_fxn, cv=n_fold_cv)\n",
    "\n",
    "#Fit the model\n",
    "rf_gs.fit(ml_df_train, training_labels)\n",
    "\n",
    "#Select the best model (this selects the parameter set with the best mean score across cv splits)\n",
    "rf_best = rf_gs.best_estimator_\n",
    "print('Parameters of best model:', rf_gs.best_params_)\n",
    "\n",
    "print('Cross validation scores using that model:',\\\n",
    "      cross_val_score(rf_gs.best_estimator_, ml_df_train, training_labels,\\\n",
    "                cv=n_fold_cv, scoring=scoring_fxn))\n",
    "\n",
    "joblib.dump(rf_best, base_dir+'rf_best.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving (?) cross-validation with pre-defined arrays of indices and repeated cross-validation\n",
    "\n",
    "Note that this will take considerably (4x) longer than the above code assuming I stick with 5-fold cross-validation above and `n_repetitions=20` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2 528 106\n"
     ]
    }
   ],
   "source": [
    "###Pre-defining n separate train/test splits\n",
    "n_repetitions = 20\n",
    "train_frac = 0.5/0.6 ###This essentially defines the training and validation set sizes\n",
    "\n",
    "###This works by selecting iloc's\n",
    "listy = list(range(0, ml_df_train.shape[0]))\n",
    "train_lists = []\n",
    "val_lists = []\n",
    "for i in range(n_repetitions):\n",
    "    random.Random(42+i).shuffle(listy)\n",
    "    train_lists.append(listy[:int(len(listy)*train_frac)]) ###Select from beginning of list up to cut-point\n",
    "    val_lists.append(listy[int(len(listy)*train_frac):]) ###Select cut-point onwards\n",
    "###Zip these two together (and make sure I did it correctly!)\n",
    "cv_splits = list(zip(train_lists, val_lists))\n",
    "print(len(cv_splits), len(cv_splits[0]), len(cv_splits[0][0]), len(cv_splits[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432 20\n",
      "2432 2\n",
      "2432 2\n",
      "({'bootstrap': True, 'class_weight': 'balanced_subsample', 'max_depth': 12, 'min_samples_leaf': 1, 'n_estimators': 10}, (0.9256198347107438, 0.9523809523809524, 0.99009900990099, 0.9661016949152542, 0.9739130434782608, 0.975609756097561, 0.9921259842519685, 0.9821428571428572, 1.0, 0.9661016949152542, 0.9557522123893805, 0.9714285714285714, 0.972972972972973, 0.9655172413793104, 0.9767441860465117, 0.970873786407767, 0.9908256880733944, 0.9836065573770492, 0.9672131147540983, 0.9649122807017544))\n",
      "({'bootstrap': False, 'class_weight': 'balanced_subsample', 'max_depth': 40, 'min_samples_leaf': 1, 'n_estimators': 80}, (0.959349593495935, 0.9606299212598426, 0.99009900990099, 0.9747899159663865, 0.9739130434782608, 0.975609756097561, 0.9921259842519685, 0.9911504424778761, 1.0, 0.983050847457627, 0.9642857142857142, 0.9714285714285714, 0.972972972972973, 0.9743589743589743, 0.9846153846153847, 0.9807692307692307, 0.9908256880733944, 0.991869918699187, 0.9752066115702479, 0.9739130434782608))\n"
     ]
    }
   ],
   "source": [
    "###Perform grid search using all the same parameters as previously defined\n",
    "rf_AJH = RandomForestClassifier()\n",
    "rf_gs_AJH = GridSearchCV(rf_AJH, params_rf, scoring=scoring_fxn, cv=cv_splits) ###Provide cv with list\n",
    "\n",
    "#Fit the model\n",
    "rf_gs_AJH.fit(ml_df_train.values, training_labels.values) ###Run on the values\n",
    "\n",
    "###Find the model with the highest minimum accuracy across all n_repetitions of cross-validation\n",
    "listy = list(zip(*[rf_gs_AJH.cv_results_['split{}_test_score'.format(i)] for i in range(n_repetitions)]))\n",
    "print(len(listy), len(listy[0]))\n",
    "listy = list(zip(*[rf_gs_AJH.cv_results_['params'], listy]))\n",
    "print(len(listy), len(listy[0]))\n",
    "listy = sorted(listy, key=lambda x: min(x[1]))\n",
    "print(len(listy), len(listy[0]))\n",
    "print(listy[0])\n",
    "print(listy[-1])\n",
    "best_params = listy[-1][0]\n",
    "rf_min_AJH = RandomForestClassifier(**best_params)\n",
    "rf_min_AJH.fit(ml_df_train, training_labels)\n",
    "\n",
    "joblib.dump(rf_min_AJH, base_dir+'rf_highMinAJH.joblib'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'class_weight': 'balanced_subsample', 'max_depth': 40, 'min_samples_leaf': 1, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
