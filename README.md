# Compiling and/or updating your own model will require the steps listed here. 

Step 1 pertains to one particular phage training data set and will probably not need to be run ever again. As a better / more expansive training set one day becomes available, it is likely that processing the data for this hypothetical set will require different code from what is used in Steps 1 and 2, but that code should essentially do the same thing.

If you're not re-downloading any data files (this can take some time), the whole pipeline can be run top-to-bottom on most common laptops (typing on a 2013 macbook pro) in an hour or two. I will re-iterate this later, but be sure that you have HMMER installed and accessible in the system path if you're planning on running the whole pipeline (code relies on "hmmbuild" and "hmmsearch").  

1. (ignore if only updating conserved domains) Step through `1-compile_phage_training_data.ipynb`. This should run with no errors and if you have downloaded/cloned the full repository this should do basically nothing except replace/re-write some files that were already there.

2. (optional: Download and replace the `cddid.tbl` file contained within `../Data/protein_domain_data/` to use an updated database. See link in the referenced notebook that follows.) Next, step through the notebook titled `2-compile_search_families.ipynb`. Depending on your goals here, you *may* want to ensure that you've deleted all existing `.afa` (and `.hmm` files, if they've been built) or alter the code accordingly to place an updated database in a new folder. In default formatting, the code will not re-download existing `.afa` files even though more recent updates to CDD may expand the size/quality of these files.

3. Now step through `3-hmmer_time.ipynb`. This will first create `.hmm` files for each of the protein domain `.afa` files. It is likely that if you have any issues running through this code it will occur here. This portion of the pipeline relies on HMMER, so to check if you have this software installed and accessible just type `hmmbuild` in your terminal. You should **not** see any variation of "command not found". If you do see this message, you'll need to install HMMER (see: http://hmmer.org/, but note that if you are using anaconda you might want to instead visit: https://anaconda.org/bioconda/hmmer). The installation should be accessible in your system path, but experienced useres may with to alter this code in order to point to a local install. This notebook then uses `hmmsearch` to create output files (containing information for all of the protein domains) for each of the viral sequences. This may take an hour or two to run, but since everything that we're working with is fairly small it did not seem worth trying to parallelize / speed up any computations. Note that I'm also running `hmmsearch` with defaults and it may be wise to tweak some sensitivity parameters at a future date/update. 

4. Now we're ready for the fun part and can step through the notebook `4-build-ml-classifier.ipynb`. This compiles full data tables from the `hmmsearch` output, splits data into training and testing sets, and ultimately fits (then saves) a Random Forest classifier.

5. To assess how well it all works, users can finally step through `5-assess-classifier.ipynb` (part of this code will require stepping through `cluster_seqs.ipynb` as well)
